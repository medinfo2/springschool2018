{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a training set from the LUNA 2016 data\n",
    "\n",
    "We are going to use the nodule locations as given in annotations.csv and extract three transverse slices that contain the largest nodule from each patient scan. Masks will be created for those slices based on the nodule dimensions given in annotations.csv. The output of this file will be two files for each patient scan: a set of images and a set of corresponding nodule masks. The data from the LUNA 2016 challenge can be found at https://luna16.grand-challenge.org/\n",
    "\n",
    "First we import the necessary tools and find the largest nodule in the patient scan. There are multiple nodule listings for some patients in annotations.csv. We're using a pandas DataFrame named df_node to keep track of the case numbers and the node information. The node information is an (x,y,z) coordinate in mm using a coordinate system defined in the .mhd file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import csv\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "file_list=glob(luna_subset_path+\"*.mhd\")\n",
    "#####################\n",
    "#\n",
    "# Helper function to get rows in data frame associated \n",
    "# with each file\n",
    "def get_filename(case):\n",
    "    global file_list\n",
    "    for f in file_list:\n",
    "        if case in f:\n",
    "            return(f)\n",
    "#\n",
    "# The locations of the nodes\n",
    "df_node = pd.read_csv(luna_path+\"annotations.csv\")\n",
    "df_node[\"file\"] = df_node[\"seriesuid\"].apply(get_filename)\n",
    "df_node = df_node.dropna()\n",
    "#####\n",
    "#\n",
    "# Looping over the image files\n",
    "#\n",
    "fcount = 0\n",
    "for img_file in file_list:\n",
    "    print \"Getting mask for image file %s\" % img_file.replace(luna_subset_path,\"\")\n",
    "    mini_df = df_node[df_node[\"file\"]==img_file] #get all nodules associate with file\n",
    "    if len(mini_df)>0:       # some files may not have a nodule--skipping those \n",
    "        biggest_node = np.argsort(mini_df[\"diameter_mm\"].values)[-1]   # just using the biggest node\n",
    "        node_x = mini_df[\"coordX\"].values[biggest_node]\n",
    "        node_y = mini_df[\"coordY\"].values[biggest_node]\n",
    "        node_z = mini_df[\"coordZ\"].values[biggest_node]\n",
    "        diam = mini_df[\"diameter_mm\"].values[biggest_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the nodule position in the mhd files\n",
    "\n",
    "The nodule locations are given in terms of millimeters relative to a coordinate system defined by the CT scanner. The image data is given as a varying length stack of 512 X 512 arrays. In order to translate the voxel position to the world coordinate system, one needs to know the real world position of the [0,0,0] voxel and the voxel spacing in mm.\n",
    "\n",
    "To find the voxel coordinates of a nodule, given its real world position, we use the GetOrigin() and GetSpacing() method of the itk image object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itk_img = sitk.ReadImage(img_file) \n",
    "img_array = sitk.GetArrayFromImage(itk_img) # indexes are z,y,x (notice the ordering)\n",
    "center = np.array([node_x,node_y,node_z])   # nodule center\n",
    "origin = np.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\n",
    "spacing = np.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\n",
    "v_center =np.rint((center-origin)/spacing)  # nodule center in voxel space (still x,y,z ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center of the nodule is located in the v_center[2] slice of the img_array. We pass the node information to the make_mask() function and copy the generated masks and the image for the v_center[2]slice and the slice above and below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i_z in range(int(v_center[2])-1,int(v_center[2])+2):\n",
    "    mask = make_mask(center,diam,i_z*spacing[2]+origin[2],width,height,spacing,origin)\n",
    "    masks[i] = mask\n",
    "    imgs[i] = matrix2int16(img_array[i_z])\n",
    "    i+=1\n",
    "np.save(output_path+\"images_%d.npy\" % (fcount) ,imgs)\n",
    "np.save(output_path+\"masks_%d.npy\" % (fcount) ,masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the make_mask() function it is worth noting that the mask coordinates have to match the ordering of the array coordinates. The x and y ordering is flipped. See the next to last line in the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def make_mask(center,diam,z,width,height,spacing,origin):\n",
    "    ...\n",
    "    for v_x in v_xrange:\n",
    "        for v_y in v_yrange:\n",
    "            p_x = spacing[0]*v_x + origin[0]\n",
    "            p_y = spacing[1]*v_y + origin[1]\n",
    "            if np.linalg.norm(center-np.array([p_x,p_y,z]))<=diam:\n",
    "                mask[int((p_y-origin[1])/spacing[1]),int((p_x-origin[0])/spacing[0])] = 1.0\n",
    "    return(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we collect more slices from each scan?\n",
    "\n",
    "Since the nodule locations are defined in terms of spheres, and the nodules are irregularly shaped, slices near the edges of the spheres may contain no nodule tissue. Using such slices would contaminate the training set with false positives. For this segmentation project, there is probably an optimal number of slices through a nodule that one should incorporate. For simplicity, we stick to 3 and only pull the slices centered on the largest nodule.\n",
    "\n",
    "Check to make sure the nodule masks look as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "imgs = np.load(output_path+'images_0.npy')\n",
    "masks = np.load(output_path+'masks_0.npy')\n",
    "for i in range(len(imgs)):\n",
    "    print \"image %d\" % i\n",
    "    fig,ax = plt.subplots(2,2,figsize=[8,8])\n",
    "    ax[0,0].imshow(imgs[i],cmap='gray')\n",
    "    ax[0,1].imshow(masks[i],cmap='gray')\n",
    "    ax[1,0].imshow(imgs[i]*masks[i],cmap='gray')\n",
    "    plt.show()\n",
    "    raw_input(\"hit enter to cont : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image on the top left is the scan slice. The image on the top right is the node mask. The image on the bottom left is the masked slice, highlighting the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
